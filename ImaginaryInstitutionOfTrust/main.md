---
Title: The Imaginary Institution of Trust
Title: The Failure of Technological Governmentality and the Persistence of Society
Author: Marcell Mars & Tomislav Medak
---

# Trust and technosolutionism

In order to establish a common successful communication Alice and Bob should trust the communication medium (language if in person or communication channel in between them if remote) and to certain degree, many would expect, they should trust to each other too. If this was the beginning of the paper in cryptography everyone in the field would know that Alice and Bob have no specific personalities, no history of personal relationship(s), not elaborate future goals of their communication, no dreams nor nightmares. Nevertheless their conversation will be joined by Carol, Carlos or Charlie (always one, never two or more), Dan, Dave or David (always one of them just like C*s), very rarely Erin (and that's only if Eve is not listening to them in secret) and Frank if the conversation becomes crowdier than usual. As already mentioned Eve is frequent malicious participant in their communication. She is an eavesdropper. She can only hear what they say to each other but unlike Mallory (aka Mallet) she's not man-in-the-middle who would modify, substitute or reorder the messages from the conversation Alice and Bob &co. are having.

The cast of members[^alicebob], together with Alice and Bob, are all fictional characters, rhymed mnemonics or metaphorical anchors which through time became idiomatic template to introduce, in a more convenient way, a communication scenarios/setups in fields like cryptography, game theory and physics. It helps explaining a complex matter. An alphabetically ordered peculiar pleiad of characters with no other social context provided but the minimal, precisely specified role in a communication scenarios in some, interesting, way portrays the community in which is situated. A community which is able to be bootstrap itself around the sole reason of exploring, sharing and working on elegant solutions for very complex problems raising from their specific problem domain. Just like in the story about Alice, Bob &co. for members of a community members the most important thing is their mastery of metaphorical anchors so complex matter can be shared through comprehensive stories, real names are not important, "kings, presidents, and voting" are rejected, what is believed is "rough consensus and running code"[^ietf]

IETF i cypherpunk.

End-to-end principle. Reliability through unreliable parts.
Public/Private Key.
Digital currency.
Blockchain as summary of many fantasies..

[^alicebob]: https://en.wikipedia.org/w/index.php?title=Alice_and_Bob
[^ietf]: https://www.ietf.org/tao.html



# Internet and the failure of technological governmentality

Techno-communitarian vision that a distributed network system designed around neutral and 'dumb' protocols with values and ‘intelligence’ built at the ends would create an adequate basis for instituting trust among globally connected communities and thus lead to a level playing field where all could participate equally in a plural global civil society obstructed the view of its embeddedness in structures of capitalist production, aggregates of economic power, ideologies of markets and governmental control. The powerful pull of these structures has directed the development of internet toward a construction of a radically unequal world marked by stupendous concentration of wealth and military control. The entrepreneurial plutocracy of Silicon Valley and the capacity of Five Eyes to blanket surveil global electronic communication are just the tip of the iceberg of that momentous act of self-deception. Platformization facilitating global arbitrage of labor, impeding automation of large labor sectors of advanced economies, integration of world-spanning logistical systems all come with increasingly ungovernable consequences of differential value capture and social polarization.

From a historical point of view, that ideologically blinkered view comes as no isolated fact. The rise of digital networks and techno-communitarianism coincided with the post-1989 moment, the Fukuyamian end of history, where the final triumph of liberalism would unstoppably be carried on by the global march of consumer culture. The collapse of the dual world-system and the faltering of socialist and anti-imperial developmental paths at the hands of structural adjustment programs resulted in an uncontested imposition of those other end-to-end engines of formalization: open markets, private property and nation states. The new liberal capitalist world of global free trade was an increasingly well-ordered domain of homogeneous regulation and procedures, where antagonistic politics was slowly receding from view. The replacement of politics with governance, of rule making with enforcement, opened a wide terrain of potential application for technological governmentality. If, as James Beniger has famously posited in The Control Revolution, the rise of modern bureaucracy and social institutions was a response to the control crisis occasioned by the growing expansion and complexity of production and distribution in the aftermath of the fossil-fuel-powered industrial revolution of the 19th century, then the 21st century could see information technologies finally supplant the inert labyrinthine world of administration, institutional regulation and statecraft by transparent protocols, distributed resources and perfect information. 

Yet this radical vision of flattened institutional power and leveled playing field transformed into its opposite once the immense capacity of digital networks for integration, co-ordination and reduction of transaction costs got absorbed into the operations of globally consolidated circuits of capital. Global production chains, online retail and marketing, international capital flows combine engines of free markets and corporate private property with communication systems to effectively catalyze unprecedented private appropriation from different quarters of the world. The network effect helps technology giants posit themselves as central entities in that transformation. As the short history of Bitcoin has patently demonstrated, while communities design technologies conducive to their shared values and interests, in this case anonymity, absence of central power and vague monetarist preference, these technologies of transparency paradoxically serve to obscure and enable the aggregates of material economic power. Bitcoin landscape is dominated by fixed capital of mining farms and spectacular currency speculation bubbles. It’s no level playing field. True, internet lowers the barrier to participation in global communication and commerce. But then participation in communities is poor man’s internet. Rare ones that have any clue how such complex technologies work can strike gold, but the result is that corporate power grows immensely. There’s almost a bifurcation: for the working class the internet is means of communication and participation, for the ruling class it is a factor of production and accumulation. Compared to the thirty years of post-WWII economic boom that saw the equalization of income, build out of public infrastructure and consolidation of welfare, now engines of end-to-end formalization combine to bypass the institutional control of accumulation and de-generalize the access to wealth. Eight wealthiest men own more than the poorer half of the humanity. Five out of those eight come from the technological sector. If it weren’t for the China’s manufacturing boom and lifting out of poverty as effect of highly centralized economic policy, the global inequality would be radically growing.

The last twenty years of technological governmentality have seen a number of crises arise amidst the capitalist world-system – global economic crisis, climate change crisis, housing crisis, surveillance crisis to name just a few. Technologies of trust built for the interaction of communities have failed in the face of polarizing values and interests, yet they have also served to undercut social institutions that have been built to mediate and strike generalizable solutions to problems of complexity, uncertainty, asymmetry, conflicting interests and values among diverging communities. In doing so, they have also undercut the vision of society – conflictual and plural – as a space of agency where a utopian imaginary can be meaningfully, generalizably and consequentially direct collective development beyond a crisis. While the communities and their formalizing visions of end-to-end engines were blossoming, the social was slowly withering away.


# Governing crisis by technology

Nowhere is the delegation of agency to end-to-end formalization more alarmingly patent than in the climate change policy arena. In the post-socialist world of governance, de-carbonization scenarios can only be politically negotiated with the engines of technologies and markets. Rapid de-carbonization through massive deployment of renewables, investments into energy efficiency and carbon taxes are already impossible to negotiate, let alone the interventions that would stymie growth and bring global economy within the limits of planetary bioregenerative capacities, which the humanity cumulatively is overstepping by a factor of 1.6 and in some places like the US or the Gulf states by a factor of 4 to 6. Because radical de-carbonization would have to burst asunder the modern idol of non-intervention and could have potentially negative effects on competitive advantages of national economies, decision makers know such interventions can be politically costly and are thus rather entering a moral hazard by betting on barely developed technologies.

In a recent analysis of the de-carbonisation scenarios that form the basis for Paris Agreement voluntary pledges, Kevin Anderson and Glen Peters have discovered that a continued commitment to high carbon emissions is hidden behind a large-scale deployment that negative emission technologies. These are technologies that would allow to capture and sequester CO2 that already exists in the atmosphere or will be created in the process of future power generation, thus extending the emissions budget of states making the pledges. In the de-carbonization scenarios the CCS technologies would offset the emissions from continued use of coal, gas and oil by up to 90% and according to the IPCC contribute up to 55% of emissions-cutting effort by 2100.  However, the problem is that CCS technologies have mostly proven ineffective and would demand enormous swaths of land – two times the area of India – to be converted into biomass, with huge adverse effects on land use, food security and biodiversity. Likely an impossible scenario. With this belief in a technological fix we might be locked to a global temperature rise well above 2.7ºC.

While the technological governmentality in the post-1989 period might have been in line with the shift of global system of social governance, now it is at odds with the situation of crisis that this system has catalyzed over the last two decades. The situations of crisis are ‘”postnormal” situations where facts are uncertain, values in dispute, stakes high and decisions urgent’, as Jerry Ravetz succinctly defines them. In enormous diplomatic efforts to reach an agreement on carbon emissions within the United Nations Framework Convention on Climate Change in Paris and later efforts to get countries to commit and sustain their voluntary pledges, we witness both the return of institutional politics and the denial of it. Obviously there’s no easy way to commit to deep de-carbonization and get numerous countries rich in fossil fuels to give up on the source of their competitive advantage and economic development, while keeping the framework of international competition intact. Thus negative emission technologies are a perfect vehicle to defer the resolution of the actual conflict to a later date – hoping that it will markets and technology, not the climate catastrophe that will precipitate that denouement.

As Jerry Ravetz and his long-standing collaborator Silvio Funtowicz have proposed, the situations of crisis require a shift from normal science based on models that provide predictions using standard scientific methods to a post-normal science that can take into account both the unknown and unknowable as well as the values in dispute and conflicting stakes. The post-normal science is called for in issues of ‘environmental, development and equity policy debates’ that both require a variety of expertises and a variety of stakeholders with conflicting interests to ‘participate in assuring the quality of the scientific input’, allowing scientists ‘to articulate a configuration of “extended facts,” including a diversity of knowledge (scientific, indigenous, local, traditional), a plurality of values (social, economic, environmental, ethical), and beliefs (material, spiritual), which all together, and together with conventional “scientific facts,” inform the analysis of the problem at stake.’

Yet it is exactly such pluralist forms of institutional deliberation that the post-normal science proposes, with its commitment to the unknown, the conflictual and the uncertain, that the global governance structures cannot even begin to consider. While more universal imaginary, institutional agency and global redistributive politics is required to accommodate experimentation with processes that would lead to creating a global social metabolism that could guarantee a sustainable future of humanity on the planet, the international and national institutional frameworks cannot but remain in thrall of the simplifications of the free market and the technological development, hoping that the subsidies might be all they need to do.

# Collapse of Technological Control

As a cautionary tale of failure that social institutions end up  once they get sucked down into technological governmentality trap, it is worth looking back to the early development of cybernetics. Ravetz in his discuss of scientific model as metaphor distinguishes two types of scientific modeling in decision making. One that is premised on predicting the future, where uncertainty is considered a failure. This type modeling is full of metaphorical language replete with assumptions stemming from hard sciences and economics. ‘Overarching metaphors like "growth" are translated into particular sorts of social-scientific language, and are then given very particular sorts of policy implications.’ The other is scientific modeling that embraces the plurality of metaphors we use to explain the complex reality and the unknowablity of the future to grapple with challenges of decision making. Metaphors and imaginary are thus part and parcel of both science and politics in the face of an impeding crisis. 

It is along the lines of the first type of metaphor that the cybernetic modeling of Cold War conflict has been famously analyzed by Paul Edwards. In the wake of WWII, during which the nascent discipline cut its teeth in the ballistics research, applying mathematics and electrical engineering to the task of predicting trajectory of a flying target, cybernetics emerged as a unified framework of analysis of control and communication loops that could be equally applied to machines, biological organisms and social structures. The world was suddenly amenable to mathematical analysis and technological control. Advances in computing during the 1950s and 1960s have placed cybernetic systems at the center of the Cold War control room. Cybernetics provided both a tool to devise military strategies in the nuclear conflict and a foundation for Operations Research and System Analysis in the planning of military resources. However, cybernetic systems provided not only the technological and analytical foundation, but also the metaphor that fundamentally shaped the imaginary of the Cold War – that of a closed world that could be contained, analyzed, simulated and controlled. This metaphor formed the world-view of American military and political doctrine where the Soviet Union was seen as a closed society, the capitalist world as a system shielded from communism and Cold War conflict had the ultimate goal of opening up the Soviet zone of influence to the free market. The ‘closed-world discourse’ encompassed techniques of scientific modeling, advanced technologies, practices of simulation, rule-oriented political strategies, ideological fixations and ‘a language of systems, gaming, and abstract communication and information that relied on formalisms to the detriment of experiential and situated knowledge.

Developed to analyze, simulate and carry out scenarios of the nuclear containment between the two nuclear superpowers, the vision of technologically controllable conflict collapsed in the conditions of guerrilla warfare in Vietnam. In 1967 the US military set up a network of sensors in the jungle along the vital Ho Chi Min Trail notifying the control center in Thailand and activating the response from patrolling air bombers. While the US soldiers were incentivized with promotions to overreport successful strikes, leading to fabricated statistics and skewed feedback, the Vietnamese and Latian forces learned how to confuse the sensors by using ‘tape-recorded truck sounds and bags of urine’. In spite of the enormous statistical success of Igloo White, they managed to transport forces and tanks to carry out major operations in South Vietnam in 1972. The American doctrine fell prey to its own blind faith in technological capacity to contain the war-zone. The technocratic trust in an electronically controllable battlefield and statistically driven war planning that dominated the US military and foreign policy doctrine just collapsed over the unknowns and uncertainties of a low-intensity non-technological guerrilla warfare.

There are obvious parallels to be drawn between the cybernetic warfare of the Cold War period and the technological imaginary of the post-1989 period. A small community of scientists and technologists, mostly at RAND and Pentagon, constructed a world that conformed to their values and interests, a closed world upon which they could unleash their tools. There are also, however, genealogical lineages that emerged from the Cold War in the form of microeconomics, game theory, systems analysis that would ultimately win the game of attrition against its Soviet opponent and shape the economic world of the present. The development of computing over the last two decades has undergone radical transformations with the massive deployment of digital networks, the ubiquity of computer systems and the growing centralization of computing. This has both enabled and was enabled by the post-1989 global integration of the free market envisioned in the strategy of containment. Cybernetic systems of the present catalyze an accelerated capitalist world-systems that is teetering at the brink of a systemic crisis and a planetary collapse. The world of the present is marked by enormous socio-economic disparities between ever smaller segment of global population in the North and much larger impoverished global populations based in Asia and Africa. While the North dominates the economic, technological and military landscape, the institutional landscape is increasingly left to wither away so as to not face the fundamental issue how we can reach a more equitable and sustainable global system.

# Paths not taken
Norbert Wiener was reluctant to expand the cybernetics to social organizations for ethical reasons. While he saw understood that the social systems could be analyzed in the same terms as machines and organisms and potentials for cybernetic architecture to enable the interaction of humans with a growingly complex environment, he also understood that it could be also put to use toward forms of social development that were contrary to his humanist goals. In fact, in 1949 Wiener penned a letter to Walter Reuther, the president of the Union of Automobile Workers trade union, warning of the threat of automation on labor and placing himself at the service of opposing that development. While the post-war push for automation, driven largely by the pressure of US military to stifle labor resistance, proved to be of relatively limited effect, cybernetics was not all the US military community desired it to be. Even though their vision had prevailed, it had prevailed rather by the merit of superior economic and military power. The WWII had left the US in a superior position over its allies and its enemies, and by 1989 the US has used its position to secure its hegemony. Even in the present the US is the greatest military power and largest consumer market of the world, holding domination over the global politics and economic development in spite of the slow-down of its crumbling leading role.

The historian of technology Roland Kline has suggested that there’s a disunity of cybernetics. While the interdisciplinary endeavor of organic, second-order cybernetics advocated by the likes of Wiener, Bateson and Maturana slowly lost its appeal by 1970s, the machine, first-order cybernetics of the engineers eventually prevailed and paved the way for microcomputing and digital networks. It is this machine cybernetics, which black boxes the social component in the organization of technological systems, that continues to dominate to this day. However, other paths stood open to the development of cybernetics outside of the US that would remain for various reasons not taken.

Two historic attempts to build out nation-wide cybernetic systems outside the US were revisited by Eden Medina and Ben Peters. Medina has analyzed the Project Cybersyn initiated by Salvador Allende’s government and the British cybernetician Stafford Beer in Chile in 1971. Medina insists that unlike the US military’s concern to design a contained, top-down command and control system, Beer and Popular Unity democratically elected socialist government were interested in designing a system that would maintain the stability and central coordination of Chile’s economy while allowing adaptation to the decentralized signals coming from the worker-managed nationalized enterprises. Beer’s cybernetics set itself apart from Wiener’s by pragmatically analyzing the stability of a system achieved by the flexibility and adaptation of its subsystems. The Project Cybersyn consisted of three components – feedback channels for data collection from state-controlled enterprises, mainframe computer to statistically predict economic performance of the system and a control room. However, system’s development would come to an abrupt end in 1973 with the US-assisted coup that deposed Allende’s government and unleashed the first neoliberal experiment onto the world.

Following up the initial account on the development of cybernetics in the Soviet Union provided by Slava Gerovitch, Peters has revisited the plethora of proposals put forward between late 1950s and 1980s to develop a nationwide network to manage the planned Soviet economy and enable citizen communication. In the Soviet heterarchical model of economic management, with competences divided between the Politbureau and various branches of the bureaucracy, the projects aimed at networking the enterprises with the planners eventually all got bogged down because of the competition between those various branches of bureaucracy and, as David Lebovitz points out in his analysis of competing modes of production in the Soviet Union, the competition between managers and bureaucracy. Ironically, the internet in the US saw the light of the day for reasons that its development was institutionally coordinated, while the Soviet failed because it’s was competitive and contested – indicating what future might be in store for the commercialized, deregulated and de-institutionalized internet dominated by large, mostly US monopolies we got today.

These alternative paths never taken might have led to a very different technological environment of digital networks than the one we’ve got today. The technological environment that we’ve got today is no mere result of technological development, but also the result of actions by the US hegemony that extends into the present moment. Now that this hegemony has ushered us in into a number of crises that threaten social and existential stability for the large segments of the global population, the time has come to re-institutionalize its engines of formalization.


-- developer communities are key to pluralism and dynamism of technological development, institutional governance models tend to overlook that dynamic, however in order to historicize the trajectory of technological development that perspective is equally formative and necessary -- i.e. what are the visions, desires and imaginaries of developers

-- given the paths not taken, technological development historically remained within the horizon of technological optimisation that benefits the community and occludes the view of the pluralism and antagonism of plurality of communites and positions that form the society 


# Limits of Formalisation
	-- engines of formalisation, end-to-end trust: tech, market, property
	-- intellectual property + internet tech
		-- copyright emerged as complement and autonomy from autocratic control and patronage
		-- formalization of property: contracting of creative labor & construction of authorship --> rise of star system & domination of cultural industries
		-- in science and in digital enabled supercharged publishers and hapless scientists
		-- elsevier vs science hub
	-- custodians.online --> shadow libraries



# Institutions & Trust
	-- social definition of trust: shared values + competency + priority of common interest (in a situation where there's conflict of interests)
	-- institution as care for the common(s)
	-- public library & university: institutional setups to tackle complexity, uncertainty, unknowns, asymmetry, conflicting interests and values, social collapse
	-- compromised institutions
	-- transformation of instititions, institutions as a mechanism of transformation
